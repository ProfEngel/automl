{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ4vvrF0w7x3"
      },
      "outputs": [],
      "source": [
        "#Installation aller nötigen Python-Bibliotheken um ein AutoML auf einen Datensatz mittels Streamlit-GUI anzuwenden\n",
        "%%capture\n",
        "!pip install -q pillow streamlit plotly pycaret ydata_profiling pandas streamlit_pandas_profiling urllib3\n",
        "\n",
        "!npm install localtunnel\n",
        "print(\"Bibs installiert!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa2IgdSPV9_L",
        "outputId": "5425a98d-9111-481a-e75e-59e4d4cae6bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "#Fertige Version zum Schreiben für die app.py (Dafür den # vor writefile app.py entfernen)\n",
        "%%writefile app.py\n",
        "\n",
        "\n",
        "from operator import index\n",
        "from PIL import Image\n",
        "import streamlit as st\n",
        "from pycaret.regression import *\n",
        "from pycaret.classification import *\n",
        "import ydata_profiling\n",
        "import pandas as pd\n",
        "from streamlit_pandas_profiling import st_profile_report\n",
        "import urllib.request\n",
        "import os\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "target_column = None\n",
        "\n",
        "#Seitenparameter\n",
        "#im = Image.open(\"https://github.com/ProfEngel/automl/blob/133d475fd8cb5574e682fa57d1d0527535f49a32/favicon.ico\")\n",
        "#im = Image.open(\"favicon.ico\")\n",
        "st.set_page_config(\n",
        "    page_title=\"AutoML by ProfEngel\",\n",
        "    #page_icon=im,\n",
        "    layout=\"wide\",)\n",
        "\n",
        "\n",
        "# Seitenleiste\n",
        "with st.sidebar:\n",
        "    #st.image(\"https://github.com/ProfEngel/automl/blob/133d475fd8cb5574e682fa57d1d0527535f49a32/profengel_logo.png\")\n",
        "    #st.image(\"profengel_logo.png\")\n",
        "    st.title(\"AutoML by ProfEngel\")\n",
        "    choice = st.radio(\"Navigation\", [\"Upload\",\"Profiling\",\"Modelling\",\"Download\",\"Prediction\"])\n",
        "    st.info(\"Mit diesem Tool kann man einen Datensatz mittels dem AutoML-Framework von Pycaret untersuchen. Dabei können verschiedene Protokolle, Diagramme, Vorverarbeitungsprozesse, diverse Modelle evaluiert und schlussendlich das bestmöglich trainierte Modell heruntergeladen werden. Alles vollautomatisch, bzw. auf Wunsch nach Anpassung.\")\n",
        "\n",
        "# 1. Datensatz beziehen\n",
        "# Hauptauswahl für den Benutzer\n",
        "source = st.radio(\"Möchten Sie einen Datensatz aus einer Onlinequelle verwenden oder einen Datensatz hochladen?\", (\"Onlinequelle\", \"Hochladen\"))\n",
        "\n",
        "# Überprüfen Sie, ob die Datei existiert und löschen Sie sie, falls sie existiert\n",
        "if os.path.exists(\"dataset.csv\"):\n",
        "  os.remove(\"dataset.csv\")\n",
        "\n",
        "# Wenn der Benutzer einen Datensatz aus einer Onlinequelle verwenden möchte\n",
        "if source == \"Onlinequelle\":\n",
        "    st.title(\"Onlinequelle angeben\")\n",
        "    online_source = st.text_input(\"Bitte geben Sie den direkten Link zum CSV-Datensatz ein:\")\n",
        "    if online_source:\n",
        "        try:\n",
        "            urllib.request.urlretrieve(online_source, 'dataset.csv')\n",
        "            df = pd.read_csv('dataset.csv')\n",
        "            st.dataframe(df)\n",
        "        except:\n",
        "            st.error(\"Fehler beim Herunterladen oder Lesen der CSV-Datei. Bitte überprüfen Sie den Link.\")\n",
        "\n",
        "# Wenn der Benutzer einen Datensatz hochladen möchte\n",
        "elif source == \"Hochladen\":\n",
        "\tst.title(\"Datensatz hochladen\")\n",
        "\tfile = st.file_uploader(\"Bitte laden Sie Ihren Datensatz hoch:\")\n",
        "\tif file:\n",
        "\t\tdf = pd.read_csv(file, index_col=None)\n",
        "\t\tdf.to_csv('dataset.csv', index=None)\n",
        "\t\tst.dataframe(df)\n",
        "\n",
        "# 2. Profiling\n",
        "if choice == \"Profiling\":\n",
        "    st.title(\"Exploratory Data Analysis\")\n",
        "    profile_df = df.profile_report()\n",
        "    st_profile_report(profile_df)\n",
        "\n",
        "\n",
        "# 4. Modellierung\n",
        "if choice == \"Modelling\":\n",
        "\n",
        "    # Zeige immer den Profiling-Bericht, auch wenn der Benutzer zu \"Modelling\" wechselt\n",
        "    if \"profile_df\" in locals():  # Überprüfe, ob der Profiling-Bericht existiert\n",
        "        st_profile_report(profile_df)\n",
        "\n",
        "\n",
        "    # Lassen Sie den Benutzer das Target-Merkmal auswählen\n",
        "    target_column = st.selectbox(\"Welches Merkmal soll das Target sein?\", df.columns)\n",
        "\n",
        "    # Zeilen mit fehlenden Werten in der Ziel-Spalte entfernen\n",
        "    df = df.dropna(subset=[target_column])\n",
        "\n",
        "   # Checkboxen für jede Spalte (außer Target) anzeigen\n",
        "    st.subheader(\"Wählen Sie die Merkmale aus, die Sie in der Vorverarbeitung berücksichtigen möchten:\")\n",
        "    features_to_include = {col: st.checkbox(col, value=True) for col in df.columns if col != target_column}\n",
        "\n",
        "    # Filtern Sie die Daten basierend auf den Auswahlkriterien des Benutzers\n",
        "    selected_features = [col for col, include in features_to_include.items() if include]\n",
        "    df = df[selected_features + [target_column]]\n",
        "\n",
        "    # Zeige die ersten Zeilen des DataFrames\n",
        "    st.write(df.head())\n",
        "\n",
        "    # Lassen Sie den Benutzer zwischen automatischer und manueller Vorverarbeitung wählen\n",
        "    preprocessing_choice = st.radio(\"Möchten Sie die Vorverarbeitung automatisch durchführen lassen oder selbst auswählen?\", (\"Automatisch\", \"Manuell\"))\n",
        "\n",
        "    if preprocessing_choice == \"Automatisch\":\n",
        "        # Prüfen, ob die Anzahl der Merkmale (ohne das Zielmerkmal) größer als 10 ist\n",
        "        pca = st.checkbox(\"Principal Component Analysis (PCA) durchführen?\", value=True) if len(df.columns) - 1 > 10 else False\n",
        "\n",
        "        preprocessing_params = {\n",
        "            'data': df,\n",
        "            'target': target_column,\n",
        "            'session_id': 42,\n",
        "            'imputation_type': 'simple',\n",
        "            'normalize': False,\n",
        "            'remove_multicollinearity': False,\n",
        "            'multicollinearity_threshold': 0.95,\n",
        "            'polynomial_features': False,\n",
        "            'feature_selection': False,\n",
        "            'pca': pca\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        # Manuelle Auswahl der Vorverarbeitungsschritte\n",
        "        st.subheader(\"Wählen Sie die gewünschten Vorverarbeitungsschritte:\")\n",
        "        imputation_type = st.selectbox(\"Art der Imputation:\", [\"simple\", \"iterative\"], index=0, help=\"Wählen Sie die Art der Imputation für fehlende Werte.\")\n",
        "        normalize = st.checkbox(\"Daten normalisieren?\", value=True, help=\"Skaliert die Merkmale, so dass sie eine mittlere Wert von 0 und eine Standardabweichung von 1 haben.\")\n",
        "        remove_multicollinearity = st.checkbox(\"Multikollinearität entfernen?\", value=True, help=\"Entfernt Merkmale, die eine Korrelation über dem angegebenen Schwellenwert aufweisen.\")\n",
        "        multicollinearity_threshold = st.slider(\"Schwellenwert für Multikollinearität:\", 0.0, 1.0, 0.95, help=\"Merkmale mit einer Korrelation über diesem Schwellenwert werden entfernt.\")\n",
        "        polynomial_features = st.checkbox(\"Polynomiale Merkmale hinzufügen?\", value=True, help=\"Erzeugt polynomiale Merkmale bis zum angegebenen Grad.\")\n",
        "        feature_selection = st.checkbox(\"Merkmal-Auswahl durchführen?\", value=True, help=\"Wendet eine Merkmalauswahl an, um die besten Merkmale zu behalten.\")\n",
        "\n",
        "        # Prüfen, ob die Anzahl der Merkmale (ohne das Zielmerkmal) größer als 10 ist\n",
        "        pca = st.checkbox(\"Principal Component Analysis (PCA) durchführen?\", value=True) if len(df.columns) - 1 > 10 else False\n",
        "\n",
        "\n",
        "        preprocessing_params = {\n",
        "            'data': df,\n",
        "            'target': target_column,\n",
        "            'session_id': 42,\n",
        "            'imputation_type': imputation_type,\n",
        "            'normalize': normalize,\n",
        "            'remove_multicollinearity': remove_multicollinearity,\n",
        "            'multicollinearity_threshold': multicollinearity_threshold,\n",
        "            'polynomial_features': polynomial_features,\n",
        "            'feature_selection': feature_selection,\n",
        "            'pca': pca\n",
        "        }\n",
        "\n",
        "\n",
        "# Überprüfen Sie, ob target_column definiert ist und nicht None ist\n",
        "    if target_column is not None:\n",
        "\n",
        "        # Bestimmen Sie, ob das Zielmerkmal numerisch oder kategorisch ist\n",
        "        if isinstance(df[target_column].iloc[0], (int, float)):\n",
        "            # Wenn das Zielmerkmal kontinuierlich ist, verwende das pycaret.regression Modul\n",
        "            from pycaret.regression import *\n",
        "            if st.button(\"Regressionstraining starten\"):\n",
        "                reg = setup(**preprocessing_params)\n",
        "                #reg = setup(data=df, target=target_column, session_id=42)\n",
        "                setup_df=pull()\n",
        "                st.info(\"Dies ist das AutoML Training\")\n",
        "                st.dataframe(setup_df)\n",
        "                best_model = compare_models()\n",
        "                compare_df=pull()\n",
        "                st.info(\"Hier sind die AutoML Modelle\")\n",
        "                st.dataframe(setup_df)\n",
        "                st.write(f\"Bestes Modell: {best_model}\")\n",
        "                # Diagramme für Regression\n",
        "                #evaluate_model(best)\n",
        "\n",
        "                # Zeige die ersten Zeilen des DataFrames\n",
        "                st.write(df.head())\n",
        "\n",
        "                # Statistische Zusammenfassung\n",
        "                st.write(df.describe())\n",
        "\n",
        "                # Modell vorhersagen\n",
        "                predictions = predict_model(best_model)\n",
        "                st.write(\"Vorhersagen des Modells:\")\n",
        "                st.dataframe(predictions)\n",
        "\n",
        "                # Modell speichern\n",
        "                save_model(best_model, 'best_model')\n",
        "                st.write(\"Das Modell wurde als 'best_model' gespeichert.\")\n",
        "\n",
        "        else:\n",
        "            # Wenn das Zielmerkmal kategorisch ist, verwende das pycaret.classification Modul\n",
        "            from pycaret.classification import *\n",
        "            from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "\n",
        "            # Prüfen Sie die Anzahl der Beispiele in jeder Klasse\n",
        "            class_counts = df[target_column].value_counts()\n",
        "\n",
        "            # Prüfen Sie die Anzahl der Beispiele in der kleinsten Klasse\n",
        "            min_class_count = class_counts.min()\n",
        "\n",
        "            # Wenn die kleinste Klasse weniger als 6 Beispiele hat (standardmäßige n_neighbors für SMOTE + 1),\n",
        "            # verwenden Sie ROS, andernfalls verwenden Sie SMOTE.\n",
        "            if min_class_count < 6:\n",
        "                resampling_method = RandomOverSampler()\n",
        "            else:\n",
        "                resampling_method = SMOTE()\n",
        "\n",
        "            preprocessing_params['fix_imbalance'] = True\n",
        "            preprocessing_params['fix_imbalance_method'] = resampling_method\n",
        "\n",
        "            # Benutzer wählt das Kriterium aus\n",
        "            metrics = {\n",
        "                \"AUC\": \"AUC (Area Under the Curve)\",\n",
        "                \"Accuracy\": \"Accuracy\",\n",
        "                \"Recall\": \"Recall\",\n",
        "                \"Precision\": \"Precision\",\n",
        "                \"F1\": \"F1 Score\"\n",
        "            }\n",
        "            sort_metric = st.selectbox(\"Wählen Sie das Kriterium für die Modellauswahl:\", list(metrics.keys()))\n",
        "            st.info(metrics[sort_metric])\n",
        "\n",
        "            # Hyperparameter-Tuning\n",
        "            #if st.checkbox(\"Möchten Sie Hyperparameter-Tuning durchführen?\"):\n",
        "                #tuned_model = tune_model(best_model)\n",
        "                #st.write(f\"Getuntes Modell: {tuned_model}\")\n",
        "\n",
        "            if st.button(\"Klassifikationstraining starten\"):\n",
        "                clf = setup(**preprocessing_params)\n",
        "                #clf = setup(data=df, target=target_column, session_id=42)\n",
        "                setup_df=pull()\n",
        "                st.info(\"Dies ist das AutoML Training\")\n",
        "                st.dataframe(setup_df)\n",
        "\n",
        "                best_model = compare_models(sort=sort_metric)\n",
        "                st.write(f\"Bestes Modell basierend auf {sort_metric}: {best_model}\")\n",
        "\n",
        "\n",
        "                compare_df = pull()\n",
        "                st.info(\"Dies ist das ML Modell\")\n",
        "                st.dataframe(compare_df)\n",
        "                #st.write(f\"Bestes Modell: {best_model}\")\n",
        "\n",
        "                # Diagramme für Klassifikation\n",
        "                #evaluate_model(best)\n",
        "\n",
        "                # Überprüfen, ob das Modell coef_ oder feature_importances_ Attribute hat\n",
        "                if hasattr(best_model, 'coef_') or hasattr(best_model, 'feature_importances_'):\n",
        "                    st.subheader(\"Feature Importance\")\n",
        "                    feature_importance_plot = plot_model(best_model, plot='feature', save=True, verbose=False)\n",
        "                    st.plotly_chart(feature_importance_plot)\n",
        "                else:\n",
        "                    st.warning(\"Feature Importance ist für dieses Modell nicht verfügbar.\")\n",
        "\n",
        "                # Zeige die ersten Zeilen des DataFrames\n",
        "                st.write(df.head())\n",
        "\n",
        "                # Statistische Zusammenfassung\n",
        "                st.write(df.describe())\n",
        "\n",
        "                # Modell vorhersagen\n",
        "                predictions = predict_model(best_model)\n",
        "                st.write(\"Vorhersagen des Modells:\")\n",
        "                st.dataframe(predictions)\n",
        "\n",
        "                # Modell speichern\n",
        "                save_model(best_model, 'best_model')\n",
        "                st.write(\"Das Modell wurde als 'best_model' gespeichert.\")\n",
        "\n",
        "# 5. Dashboard mit Diagrammen und Tabellen\n",
        "if choice == \"Dashboard\":\n",
        "    st.title(\"Exploratory Data Analysis\")\n",
        "\n",
        "# 6. Download\n",
        "if choice == \"Download\":\n",
        "    with open('best_model.pkl', 'rb') as f:\n",
        "        st.download_button('Download Model', f, file_name=\"best_model.pkl\")\n",
        "\n",
        "# 7. Vorhersagen\n",
        "if choice == \"Prediction\":\n",
        "    st.title(\"Vorhersage auf trainiertes Modell prüfen:\")\n",
        "\n",
        "    # Lade das trainierte Modell\n",
        "    loaded_model = None\n",
        "    try:\n",
        "        loaded_model = load_model('best_model')\n",
        "    except:\n",
        "        st.error(\"Es wurde kein Modell gefunden. Bitte trainieren Sie zuerst ein Modell.\")\n",
        "\n",
        "    # Option für den Benutzer, Daten zum Vorhersagen hochzuladen\n",
        "    st.subheader(\"Daten für die Vorhersage hochladen oder eingeben\")\n",
        "    predict_file = st.file_uploader(\"CSV-Datei mit Daten für die Vorhersage hochladen:\")\n",
        "\n",
        "    if predict_file:\n",
        "        predict_df = pd.read_csv(predict_file)\n",
        "        # Stellen Sie sicher, dass die Ziel-Spalte (falls vorhanden) aus dem Vorhersage-DataFrame entfernt wird\n",
        "        predict_df = predict_df.drop(columns=[target_column], errors='ignore')\n",
        "        predictions = predict_model(loaded_model, data=predict_df)\n",
        "        st.subheader(\"Vorhersageergebnisse\")\n",
        "        st.dataframe(predictions)\n",
        "    else:\n",
        "        # Lassen Sie den Benutzer die Daten manuell eingeben, jedoch ohne die Ziel-Spalte\n",
        "        input_data = {}\n",
        "        for col in df.columns:\n",
        "            # Wir überspringen die Ziel-Spalte, da wir diese vorhersagen möchten\n",
        "            if col != target_column:\n",
        "                input_data[col] = st.text_input(f\"Geben Sie einen Wert für {col} ein:\")\n",
        "\n",
        "        # Überprüfen, ob alle Felder ausgefüllt sind\n",
        "        if all(val for val in input_data.values()):\n",
        "            input_df = pd.DataFrame([input_data])\n",
        "            predictions = predict_model(loaded_model, data=input_df)\n",
        "            st.subheader(\"Vorhersageergebnis\")\n",
        "            st.write(predictions[target_column].iloc[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqZAwX_9_7XN",
        "outputId": "4f47675b-074d-48a5-8d8f-cde18a0578c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.243.137.217\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.054s\n",
            "your url is: https://rich-boats-clap.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & curl ipv4.icanhazip.com\n",
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}